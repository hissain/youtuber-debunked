{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the dependencies from venv terminal\n",
    "\n",
    "#pip install yt-dlp\n",
    "#pip install \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yt_dlp\n",
    "\n",
    "TEST_LINK = \"https://www.youtube.com/watch?v=w4sJoZ9D1YM\"\n",
    "OUTPUR_DIR = \"downloads\"\n",
    "OUTPUT_VIDEO = \"video.mp4\"\n",
    "OUTPUT_AUDIO = \"audio\"\n",
    "\n",
    "os.makedirs(\"downloads\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydl_opts = {\n",
    "    'format': 'best',  # Download best quality\n",
    "    'outtmpl': f\"{OUTPUR_DIR}/{OUTPUT_VIDEO}\",\n",
    "    'noplaylist': True,  # Download single video, not playlist\n",
    "    'progress_hooks': [lambda d: print(f\"Downloading: {d['_percent_str']} complete\")],\n",
    "    \n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([TEST_LINK])\n",
    "    \n",
    "print(\"Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Audio as .mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',  # Select best audio format\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',  # Extract audio using FFmpeg\n",
    "        'preferredcodec': 'mp3',      # Convert to MP3\n",
    "        'preferredquality': '192',    # Set quality (kbps)\n",
    "    }],\n",
    "    'outtmpl': f\"{OUTPUR_DIR}/{OUTPUT_AUDIO}\",  # Output file name\n",
    "    'noplaylist': True,               # Download single video, not playlist\n",
    "    'progress_hooks': [lambda d: print(f\"Downloading: {d['_percent_str']} complete\")],\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([TEST_LINK])\n",
    "\n",
    "print(\"Audio download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "AUDIO_PATH = \"downloads/audio.mp3\"\n",
    "OUTPUT_DIR = \"downloads\"\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, \"transcription.txt\")\n",
    "\n",
    "print(\"Loading model and processor...\")\n",
    "model_name = \"auditi41/wav2vec2-large-xlsr-53-Bengali\"  # Update the model if required\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForCTC.from_pretrained(model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    print(f\"Loading audio file: {AUDIO_PATH}\")\n",
    "    speech_array, sampling_rate = librosa.load(AUDIO_PATH, sr=16000)\n",
    "    print(f\"Audio length: {len(speech_array)/sampling_rate:.2f} seconds\")\n",
    "\n",
    "    chunk_length = 16000 * 15  # Smaller chunks - 15 seconds\n",
    "    transcription = []\n",
    "\n",
    "    for i in range(0, len(speech_array), chunk_length):\n",
    "        print(f\"Processing chunk {i//chunk_length + 1}...\")\n",
    "        chunk = speech_array[i:i + chunk_length]\n",
    "        \n",
    "        if len(chunk) < 1000:\n",
    "            continue\n",
    "\n",
    "        inputs = processor(\n",
    "            chunk, \n",
    "            sampling_rate=16000, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\",\n",
    "            max_length=16000 * 15\n",
    "        )\n",
    "        \n",
    "        inputs = {k: v.to(device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs[\"input_values\"]).logits\n",
    "        \n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        chunk_text = processor.batch_decode(predicted_ids)[0]\n",
    "        transcription.append(chunk_text)\n",
    "        print(f\"Chunk {i//chunk_length + 1} transcribed\")\n",
    "\n",
    "    full_transcript = \" \".join(transcription)\n",
    "\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_transcript)\n",
    "\n",
    "    print(f\"Transcription saved to {OUTPUT_FILE}\")\n",
    "    print(f\"Preview: {full_transcript[:200]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error during transcription: {str(e)}\")\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BanglaASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperProcessor\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mp3_path = \"https://huggingface.co/bangla-speech-processing/BanglaASR/resolve/main/mp3/common_voice_bn_31515636.mp3\"\n",
    "\n",
    "model_path = \"bangla-speech-processing/BanglaASR\"\n",
    "\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_path)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_path)\n",
    "processor = WhisperProcessor.from_pretrained(model_path)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "\n",
    "\n",
    "speech_array, sampling_rate = torchaudio.load(mp3_path, format=\"mp3\")\n",
    "speech_array = speech_array[0].numpy()\n",
    "speech_array = librosa.resample(np.asarray(speech_array), orig_sr=sampling_rate, target_sr=16000)\n",
    "input_features = feature_extractor(speech_array, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "\n",
    "# batch = processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "predicted_ids = model.generate(inputs=input_features.to(device))[0]\n",
    "\n",
    "\n",
    "transcription = processor.decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "print(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torchaudio\n",
    "print(torchaudio.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
